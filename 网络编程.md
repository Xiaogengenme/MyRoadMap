## RPC协议

#### RPC协议是什么？

远程过程调度

rpc是一种远程调度的方法，规则，包括了传输协议和序列化方法两个部分

#### RPC的执行流程是什么？

在client调用和client socket发送中间和服务器端的socket和方法调用中间，添加client stub和server stub，对调用的请求进行序列化和反序列化，序列化是将调用的信息包括方法名和参数序列化成适用于RPC发送解析的数据格式。

#### ProtoBuffer的序列化方法：

在项目中定义proto文件，proto文件中有一个NaviConf.proto文件，定义了一些配置信息



## TCP基础知识

![img](https://raw.githubusercontent.com/Xiaogengenme/ImagesResource/main/20210817151638.jpg)

## TCP：套接字与三次握手

建立连接时服务器端的bind**：将自己的服务能力绑定在一个众所周知的地址和端口上**

服务端阻塞：bind之后的listen操作会将socket转化为服务端socket，之后**服务端会阻塞在accept方法上**

> socket和服务端socket有啥区别？

基于后面的学习我了解到socket在最开始创建的时候默认为一个主动套接字，listen操作会将一个主动套接字转化为一个被动套接字。

服务端准备就绪后，客户端流程就是：**初始化socket，再执行connect方法向服务器的地址和端口发送建立连接请求，进行三次握手。**

#### 套接字

IPv4地址长度32位：AF_INET + 16bit 端口号 + 32 bit IPV4地址 + 占位符 = 固定16字节

IPv6的地址128位：AF_INET6 + 16bit 端口号 + 32bit 流标（不管） + 128bit IPV6地址 + 32 bit范围ID = 固定28字节

> 为什么本地套接字不需要端口号，ipv4和ipv6套接字需要端口号呢？

本地端口号是**用于访问本地的文件系统**，并不需要端口号。而ipv4和ipv6都是需要将数据发送远程计算机的一个进程，远程计算机可能会有多个进程在进行监听，所以要使用端口号来区分不同进程。

#### 连接——服务端

##### 1. 创建套接字

`int socket(int domain, int type, int protocol)`

1. domain：ipv4、ipv6还是本地套接字等

2. type：

    a. SOCK_STREAM：字节流 TCP

    b. SOCK_DGRAM：数据报 UDP

    c. SOCK_RAW：原始套接字

3. protocol一般只设置为0

##### 2. bind

`bind(int fd, void *addr, socklen_t len)`

```c
int make_socket(uint16_t port) {
    int sock;
    struct sockaddr_in name;
    
    sock = socket(PF_INET, SOCK_STREAM, 0);
    if (sock < 0)
    {
        perror("socket");
        exit(EXIT_FAILURE);
    }
    
    /*binding过程*/
    name.sin_family = AF_INET;
    name.sin_port = htons(port);
    name.sin_addr.s_addr = htonl(INADDR_ANY);	/*通配地址*/
    
    
    if (bind(sock, (struct sockaddr*) &name, sizeof(name)) < 0) {
        perror("bind");
        exit(EXIT_FAILURE);
    }
    
    return sock;
}
```

##### 3. listen

我们一开始创建的是一个主动套接字，可以主动发起请求（调用connect函数），listen方法可以将主动套接字转化为一个被动套接字，告诉操作系统这个套接字是用来接受连接请求的。

`int listen(int socketfd, int backlog)`

其中，socketfd表示套接字描述符，第二个backlog是指可以接收的并发数目，这个参数越大，并发数目（理论上）越大

##### 4. accept

`int accept(int linstensockfd, struct sockaddr *cliaddr, socklen_t *addrlen)`

accept函数的参数listensockfd就是之前通过sock建立并通过listen转化的监听套接字，而accept函数返回的是一个用于客户端和服务器端进行数据交互的已连接套接字

#### 连接——客户端

##### 1. 创建套接字

##### 2. connect

客户端在执行connect之前没必要非得执行bind方法，因为客户端的内核会自动选择源IP地址和端口进行创建

`int connect(int sockfd, const struct sockaddr * servaddr, socklen_t addrlen)`

执行connect函数会激发TCP的三次握手过程，连接成功/失败后返回。

#### TCP的三次握手过程

### TCP TIME_WAIT

<img src="https://raw.githubusercontent.com/Xiaogengenme/ImagesResource/main/202108171518500.png" alt="img" style="zoom:67%;" />

断开TCP连接：四次挥手

当TCP连接想要关闭时，主动发起断开连接的主机1发送一个FIN报文，主机2由于在read函数中读到EOF获取到需要断开连接，进入CLOSE_WAIT状态，并发送一个ACK应答。主机2read读到EOF之后，通知应用程序主动关闭连接，发送FIN报文。

主机1接受到FIN报文后发送ACK应答，并进入TIME_WAIT状态

> 问题：主机2是不是在读到了EOF之后才知道需要断开连接的消息的？

##### 为什么要有TIME_WAIT？为什么要等待两个MSL？

1. 是为了确保被动关闭连接方能够正常关闭

    2MSL从主动发起连接方发送出ACK应答包开始计时，ACK在网络中存活的最长时间为1MSL，如果被动接收方没有收到ACK，会向主动方发送重传请求，这个重传请求在网络中的存活时间也是1MSL，最坏的情况是2MSL，如果过了2MSL没有收到重传消息，那么就可以确保被动断开连接方能够正常关闭。

2. 是为了让旧连接在网络中自然消失

    考虑这样一种场景：在原连接没有经过TIME_WAIT阶段就关闭后，主机1（主动方）再次创建了一个与原连接TCP四元组（源IP，目标IP，源端口，目标端口）相同的连接，那么原来在网络中没有传到也没有被自然丢弃的数据包就会将新连接看做是旧链接，对新的TCP连接产生影响。

    > 问题：这个时间的问题，为什么2MSL就会让所有数据报自然死亡？

##### TIME_WAIT的危害

1. 内存资源占用
2. 端口占用

##### 如何优化TIME_WAIT

1. 暴力将系统TIME_WAIT时间调小
2. SO_LINGER：修改套接字选项，不建议使用
3. 安全合理的方式：修改`net.ipv4.tcp_tw_reuse`
    1. 只适用于连接发起方（C/S模型中的客户端）；
    2. 对应的TIME_WAIT状态的连接创建时间超过1秒才可以被复用。



## 文件读写——通过socket进行读写

当TCP三次握手建立成功之后，操作系统会为每个连接建立发送缓冲区

#### write

发送缓冲区的大小通过套接字的选项来设置，当我们的应用程序调用write函数的时候，实际做的事情就是**把数据从应用程序（的内存）中拷贝到操作系统内核的发送缓冲区中**。

`ssize_t write(int socketfd, const void *buffer, size_t size)`

write方法如果正常执行，发送缓冲区足够容纳需要拷贝的数据，就正常拷贝，返回写入数据字节的大小

如果需要写入的数据还没有准备好，只能写一半，或者缓冲区的大小不够容纳，这时操作系统并不会返回或报错，而是阻塞，并进行一些处理。处理的方式不同的操作系统有所不同。

操作系统会聪明地一点点将数据缓冲区的数据打包，通过网络传输出去。但是需要注意的是，当write方法返回的时候，只代表需要写入缓冲区的数据全部被写入进去，并不代表缓冲区的数据已经全部被发出去了。

#### read

`ssize_t read(int socketfd, void *buffer, size_t size)`

read方法的参数：socketfd套接字描述符，buffer缓冲区，size读取最多多少个字节。

返回值告诉我们实际的字节数目，返回值为0代表EOF（end-of-file），这在网络中表示FIN包，要进行断开连接的处理。返回-1表示出错（阻塞IO中，非阻塞IO中情况不同）

notice：

- 对于write来说，返回成功仅仅表示数据写到发送缓冲区成功，并不表示对端已经成功收到。
- 对于read来说，需要循环读取数据，并且需要考虑EOF等异常条件。



## UDP

![img](https://static001.geekbang.org/resource/image/84/30/8416f0055bedce10a3c7d0416cc1f430.png)

UDP建立连接和发送数据的方式和TCP有不同，发送数据和接收数据分别使用sendto和recvfrom进行





## IO多路复用

**多路复用中的多路一般指的是多个网络链接，复用指的是使用同一个线程来监视多个文件描述符。**

需要进行处理的IO事件有很多种：

* 标准输入文件描述符准备好可以读
* 监听套接字准备好，新的连接建立成功
* 已连接套接字准备好可以读
* 如果一个I/O时间等待超过10秒，发生超时事件

### 1. select

`int select(int maxfd, fd_set *readset, fd_set *writeset, fd_set *exceptset, const struct timeval *timeout)`

参数：

1. maxfd：表示待测试的描述符数目+1
2. 读描述符集合、写描述符集合与异常描述符集合
3. timeout：select函数检测超时时间

select的实际例子代码：

```c
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: select01 ");
    }
    int socket_fd = tcp_client(argv[1], SERV_PORT);

    char recv_line[MAXLINE], send_line[MAXLINE];
    int n;

    fd_set readmask;
    fd_set allreads;
    FD_ZERO(&allreads);		//通过FD_ZERO初始化了一个描述符集合
    FD_SET(0, &allreads);	// 将第0号描述符（即标准输入）设置为待检测
    FD_SET(socket_fd, &allreads);	// 将第3号描述符（连接套接字描述符）设置为待检测

    for (;;) {
        readmask = allreads;	// 每次检测一轮之后，将待测试的描述符集合重置
        int rc = select(socket_fd + 1, &readmask, NULL, NULL, NULL);

        if (rc <= 0) {
            error(1, errno, "select failed");
        }

        if (FD_ISSET(socket_fd, &readmask)) {	// 使用FD_ISSET判断哪个描述符可以读了，这里判断是socket_fd连接套接字描述符可读，那么使用read函数将套接字数据读出
            n = read(socket_fd, recv_line, MAXLINE);
            if (n < 0) {
                error(1, errno, "read error");
            } else if (n == 0) {
                error(1, 0, "server terminated ");
            }
            recv_line[n] = 0;
            fputs(recv_line, stdout);
            fputs("", stdout);
        }

        if (FD_ISSET(STDIN_FILENO, &readmask)) {	// 这里是STDIN_FILENO标准输入描述符可读，那么就将标准输入的数据读入
            if (fgets(send_line, MAXLINE, stdin) != NULL) {
                int i = strlen(send_line);
                if (send_line[i - 1] == '') {
                    send_line[i - 1] = 0;
                }

                printf("now sending %s", send_line);
                size_t rt = write(socket_fd, send_line, strlen(send_line));
                if (rt < 0) {
                    error(1, errno, "write failed ");
                }
                printf("send bytes: %zu ", rt);
            }
        }
    }

}
```

而select检测是什么时候会返回，认为某个套接字描述符可读呢？

1. 套接字接受缓冲区有数据可读，如果我们使用read去执行读操作则绝对不会被阻塞
2. 收到了FIN断开连接请求，那么使用read函数进行读操作，不会被阻塞
3. 针对监听套接字，如果有已经完成的连接，此时使用accept函数去执行不会被阻塞，可以得到完成连接的套接字
4. 套接字有错误待处理，使用read进行读取返回-1

总之就是内核通知函数有套接字可以读，使用read（或accept）函数不会被阻塞时，就会调用select函数进行遍历查找。

##### notice

* select的描述符基数（遍历长度）是当前最大描述符+1
* 每次select调用完成之后要重置待测试集合

> 了解select、poll、epoll之间的区别，一个比较好的角度就是比较他们的输入参数，参数代表了他们能够监控和操作的数据结构的区别。其中，select参数主要有一个maxfd：待检测的描述符数量+1，随后是三种文件描述符集合：读描述符集合，写描述符集合和异常描述符集合。
>
> select通过循环遍历可以读的文件描述符集合，查找可以读文件描述符然后通知应用程序有数据准备好可以读。
>
> 一个文件可以读，就证明调用会阻塞的read函数（或者是建立连接的accept函数）不会被阻塞，主要几种情况就像上面写的，已连接套接字有新的数据到达，已连接套接字收到了FIN断开连接消息，监听套接字已与客户端完成三次握手连接，使用accept方法不会被阻塞，还有就是套接字出现错误信息可读。

### 2. poll

select虽然可以一定程度解决IO读写时的阻塞问题，但是也有它的问题，select的描述符数组长度是有限制的，长度为1024

poll和select相比，在内核交互的数据结构，以及文件描述符的个数限制上做了改进

`int poll(struct pollfd *fds, unsigned long nfds, int timeout)`

参数：

1. pollfd结构体：

    ```c
    struct pollfd {
        int fd;	// 文件描述符fd
        short events;	// 描述符上待检测的事件类型events：一个evnets可以通过位操作表示多个事件
        short revents;	// poll每次检测完待检测的描述符之后，不会将描述符的结果重置，而是会将结果保留在revents中
    }
    ```

events的类型大概分为两种，可读事件与可写事件，可读与可写事件的定义与select中的readset和writeset差不多，表示当前可以读取文件描述符中数据read方法不会被阻塞，写的话可以向文件缓冲区中写数据，不会被阻塞。

2. nfds就是申请检测的文件描述符的个数
3. timeout是检测超时时间：如果是个负数表示在事件发生之前会一直等待；如果是0表示不阻塞进程，立即返回；如果是>0表示超时时间

返回值：

当有错误发生时，返回-1，如果在制指定时间内无事发生，返回0，否则返回检测到的事件个数

一个基于poll的服务器模型：

```c
#define INIT_SIZE 128

int main(int argc, char **argv) {
    int listen_fd, connected_fd;
    int ready_number;
    ssize_t n;
    char buf[MAXLINE];
    struct sockaddr_in client_addr;

    listen_fd = tcp_server_listen(SERV_PORT);	// 创建一个监听套接字然后绑定在本地地址和端口，调用tcp_server_listen执行

    //初始化pollfd数组，这个数组的第一个元素是listen_fd，其余的用来记录将要连接的connect_fd
    struct pollfd event_set[INIT_SIZE];	// 初始化一个pollfd结构体数组类型的event检测数组，长度为INIT_SIZE
    event_set[0].fd = listen_fd;	// 将event_set中第一位设为监听fd，说明这一位的event用于监听listen套接字完成的事件
    event_set[0].events = POLLRDNORM;	// 将第一位的event设置为一个可读事件（假装有可读事件）

    // 将eventfd数组中的fd成员变量值都设为-1，在遍历时poll函数对于fd为-1的事件会自动略过。
    int i;
    for (i = 1; i < INIT_SIZE; i++) {
        event_set[i].fd = -1;
    }

    for (;;) {
        if ((ready_number = poll(event_set, INIT_SIZE, -1)) < 0) {	// 在这一行调用poll函数来检测事件set中的可以处理的事件，传入的参数包括事件队列本身，事件队列长度（因为poll函数会自动略过值为-1的位置所以直接传入整个长度就可以，-1表示没有事件发生会一直等待
            error(1, errno, "poll failed ");
        }

        if (event_set[0].revents & POLLRDNORM) {  // 这里使用位与操作来进行事件的识别的，poll函数的event_set中使用二进制位表示事件
            socklen_t client_len = sizeof(client_addr);
            connected_fd = accept(listen_fd, (struct sockaddr *) &client_addr, &client_len);  // 监听到一个listen请求可读事件之后，服务端执行accept方法，将之前的监听套接字转化为一个已连接套接字，这时候就需要新建新的检测事件来监听已连接的套接字

            //找到一个可以记录该连接套接字的位置，创建新的对于已连接套接字的检测
            for (i = 1; i < INIT_SIZE; i++) {
                if (event_set[i].fd < 0) {
                    event_set[i].fd = connected_fd;
                    event_set[i].events = POLLRDNORM;
                    break;
                }
            }

            // 如果已经达到了事件队列能容纳的长度，证明服务器端无法处理这么多得客户端连接
            if (i == INIT_SIZE) {
                error(1, errno, "can not hold so many clients");
            }
			// 对poll的遍历检测进行优化，如果处理了监听套接字之后，待处理的套接字个数为0，那么说明本次的遍历检测已经达到目的，可以退出执行下一次
            if (--ready_number <= 0)
                continue;
        }
        for (i = 1; i < INIT_SIZE; i++) {  // 接下来是对：没有检测到监听套接字的可读事件（无事发生）的情况进行处理
            int socket_fd;
            if ((socket_fd = event_set[i].fd) < 0)	// 如果该事件检测没有提交有效的可读事件，那么就直接跳过检测下一个
                continue;
            if (event_set[i].revents & (POLLRDNORM | POLLERR)) {  // 检测revent的事件类型是不是可读事件或者error事件
                if ((n = read(socket_fd, buf, MAXLINE)) > 0) {	// 如果有可读事件就进行读操作
                    if (write(socket_fd, buf, n) < 0) {	// 之后写回给客户端
                        error(1, errno, "write error");
                    }
                } else if (n == 0 || errno == ECONNRESET) {	// 如果读到EOF或者连接重置，那么需要执行close函数断开连接，并将连接重置为-1（不检测这一位的）
                    close(socket_fd);
                    event_set[i].fd = -1;
                } else {	// 读取数据失败
                    error(1, errno, "read error");
                }
				// 和之前一样是遍历优化处理
                if (--ready_number <= 0)
                    break;
            }
        }
    }
}

```

> poll方法接收的参数是一个pollfd结构体，和nfds：申请检测的文件描述符个数。
>
> 这个pollfd结构体将文件描述符封装起来，其中的数据元素包括文件描述符fd，文件描述符上的事件类型events（events使用二进制位来表示不同的事件是否有发生，判断读取的时候也是使用二进制位进行比较），revents
>
> poll与select最大的区别就是采用pollfd结构体之后，采用链表进行存储，突破了检测数量的限制。
>
> 还有问题就是poilfd结构体之后导致内存占用过多，每次从用户态将数据拷贝至内核态进行判断开销很大。

### 3. epoll

epoll通过监控注册的多个描述字，来进行I/O事件的分发处理，不同于poll的是，epoll不仅提供了默认的条件触发的方式，还有更强大的边缘触发（edge-triggered）方式

epoll分为三个步骤：epoll_create, epoll_ctl和epoll_wait

##### epoll_create

创建一个epoll实例，用来调用epoll_ctl和epoll_wait方法

##### epoll_ctl

`epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);`

使用epoll_create创建了epoll实例之后可以通过调用epoll_ctl来往这个epoll实例中添加或删除监控的事件。

参数：

1. epfd：刚刚使用epoll_create创建的epoll实例

2. op: 增加还是删除一个监控事件：

    * EPOLL_CTL_ADD
    * EPOLL_CTL_DEL
    * EPOLL_CTL_MOD

3. fd: 需要操作的事件的文件描述符

4. event: 表示注册的事件的类型，将事件文件描述符 + 事件类型 + 其他信息组装成一个数据结构传入epoll函数中，具体结构如下：

    ```c
    typedef union epoll_data {
         void        *ptr;
         int          fd;
         uint32_t     u32;
         uint64_t     u64;
     } epoll_data_t;
    
     struct epoll_event {
         uint32_t     events;      /* Epoll events */
         epoll_data_t data;        /* User data variable */
     };
    ```

    其中，事件类型与poll相同，有以下几种：

    * EPOLLIN：表示对应的文件描述字可以读
    * EPOLLOUT：表示对应的文件描述字可以写
    * EPOLLRDHUP：表示套接字的一段已经关闭，或者半关闭
    * EPOLLHUP：表示对应的文件描述字已经被挂起
    * EPOLLET：设置为edge-triggered或者level-triggered

#### epoll_wait

`int epoll(int epfd, struct epoll_event *event, int maxevents, int timeout)`

epoll_wait函数类似于之前的select和poll函数，调用者的进程被挂起（访问时），当内核有事件时才会继续执行。

参数：

1. epfd：epoll实例描述字

2. 第二个参数是一个数组，数组的大小由epoll_wait的返回值所确定，数组中的元素是待处理的I/O事件，其中events表示具体的事件类型，可选的值就是上面介绍过的五种。

    所包含的epoll_event中的data值就是在epoll_ctl中设置的data，用于用户空间和内核空间调用时所需

3. maxevent：epoll_wait可以返回的最大事件值

4. epoll_wait的调用超时时间，-1表示永不超时，0表示立即返回

epoll代码解析：

```c
#include "lib/common.h"

#define MAXEVENTS 128

char rot13_char(char c) {
    if ((c >= 'a' && c <= 'm') || (c >= 'A' && c <= 'M'))
        return c + 13;
    else if ((c >= 'n' && c <= 'z') || (c >= 'N' && c <= 'Z'))
        return c - 13;
    else
        return c;
}

int main(int argc, char **argv) {
    int listen_fd, socket_fd;
    int n, i;
    int efd;
    struct epoll_event event;
    struct epoll_event *events;

    listen_fd = tcp_nonblocking_server_listen(SERV_PORT);

    efd = epoll_create1(0);	// 创建一个epoll实例
    if (efd == -1) {
        error(1, errno, "epoll create failed");
    }

    event.data.fd = listen_fd;	// 设置事件检测的是一个监听套接字描述字
    event.events = EPOLLIN | EPOLLET;	// 设置所感知的事件类型：可读事件，并设置为边缘触发
    if (epoll_ctl(efd, EPOLL_CTL_ADD, listen_fd, &event) == -1) {	// 将该事件注册到epoll检测的范围内
        error(1, errno, "epoll_ctl add listen fd failed");
    }

    /* Buffer where events are returned */
    events = calloc(MAXEVENTS, sizeof(event));	// 创建events数组

    while (1) {
        n = epoll_wait(efd, events, MAXEVENTS, -1);	// 调用epoll_wait方法检测监听的文件描述符们
        printf("epoll_wait wakeup");
        for (i = 0; i < n; i++) {
            if ((events[i].events & EPOLLERR) ||
                (events[i].events & EPOLLHUP) ||
                (!(events[i].events & EPOLLIN))) {	// 这里events中的事件类型都不是我们期望的listen_fd会出现的事件类型，代表epoll出现了错误
                fprintf(stderr, "epoll error");
                close(events[i].data.fd);
                continue;
            } else if (listen_fd == events[i].data.fd) {	// 这里，events中的事件类型符合了预期，可读（代表listen_fd可以被转化为已连接套接字）
                struct sockaddr_storage ss;
                socklen_t slen = sizeof(ss);
                int fd = accept(listen_fd, (struct sockaddr *) &ss, &slen);	// 调取accept函数将listen_fd转化为已连接套接字
                if (fd < 0) {
                    error(1, errno, "accept failed");
                } else {
                    make_nonblocking(fd);	// 将这个已连接套接字设置为非阻塞类型
                    event.data.fd = fd;
                    event.events = EPOLLIN | EPOLLET; //edge-triggered
                    if (epoll_ctl(efd, EPOLL_CTL_ADD, fd, &event) == -1) {	// 再将这个已连接套接字注册到epoll检测事件队列中，方法是通过将event_data中的fd字段设置为我们要监听的fd
                        error(1, errno, "epoll_ctl add connection fd failed");
                    }
                }
                continue;
            } else {
                /*
                处理已连接套接字上的可读事件，读取字节流，编码后再回应给客户端
                */
                socket_fd = events[i].data.fd;
                printf("get event on socket fd == %d ", socket_fd);
                while (1) {
                    char buf[512];
                    if ((n = read(socket_fd, buf, sizeof(buf))) < 0) {
                        if (errno != EAGAIN) {
                            error(1, errno, "read error");
                            close(socket_fd);
                        }
                        break;
                    } else if (n == 0) {
                        close(socket_fd);
                        break;
                    } else {
                        for (i = 0; i < n; ++i) {
                            buf[i] = rot13_char(buf[i]);
                        }
                        if (write(socket_fd, buf, n) < 0) {
                            error(1, errno, "write error");
                        }
                    }
                }
            }
        }
    }

    free(events);
    close(listen_fd);
}
```

epoll通过改进的接口设计，避免了用户态-内核态频繁的数据拷贝，大大提高了系统性能。

> 🤔epoll如何通过改进接口设计，避免了用户态-内核态频繁的数据拷贝，大大提高了系统性能呢？



## 非阻塞IO

#### 阻塞与非阻塞的区别

阻塞IO是指应用程序向操作系统内核请求操作时，会被挂起，等待操作系统内核处理成功返回之后才继续执行，在这期间操作系统内核会将CPU时间切换给其他进程。

非阻塞IO是指应用程序调用非阻塞IO执行某项操作的时候，内核立即返回，不会把CPU时间片切换给其他进程

#### 非阻塞IO与异步IO的区别



### 非阻塞IO

#### 读操作

如果在读操作时数据缓冲区没有数据可读，非阻塞IO会立即返回，一般返回的是EWOULDBLOCK或者EAGAIN出错信息，而这些出错信息并不会导致返回错误，而是通过不断轮询访问是否有数据可读进行处理。

#### 写操作

如果写操作出错（一般是发送缓冲区的空间容不下需要写的字节数），那么非阻塞IO写操作的返回值（返回已写入的字节数）就比较有用了，告诉应用程序进程已经将传入数据的多少字节数写入到了发送缓冲区。（但是一般来说阻塞IO的写操作返回值一般都会和输入的一样，因为不成功它也不会返回）

**关于read和write还有几个结论，你需要把握住：**

1. **read总是在接收缓冲区有数据时就立即返回，不是等到应用程序给定的数据充满才返回。当接收缓冲区为空时，阻塞模式会等待，非阻塞模式立即返回-1，并有EWOULDBLOCK或EAGAIN错误。**
2. **和read不同，阻塞模式下，write只有在发送缓冲区足以容纳应用程序的输出字节时才返回；而非阻塞模式下，则是能写入多少就写入多少，并返回实际写入的字节数。**
3. **阻塞模式下的write有个特例, 就是对方主动关闭了套接字，这个时候write调用会立即返回，并通过返回值告诉应用程序实际写入的字节数，如果再次对这样的套接字进行write操作，就会返回失败。失败是通过返回值-1来通知到应用程序的。**

#### accept函数

当accept方法和IO多路复用select、poll等一起使用的时候，如果监听套接字上触发事件，那么就代表有新的连接建立成功了，程序可以非阻塞地调用accept函数，将监听套接字转化为已连接套接字

> 🤔为什么监听套接字一定要定义为非阻塞的？



#### connect函数

在非阻塞情况下调用套接字的connect函数，程序会立即返回一个EINPROGRESS错误。随后程序会正常进行tcp三次握手，应用程序正常进行各种初始化工作。

随后会使用select、poll等方法对连接是否建立进行状态检测。



```c
/*
可以通过非阻塞方式 + select多路复用方式实现一个服务器端
*/
```



### C10K问题

就是一个服务器如何能满足10k个用户（指很多）的同时连接服务的需求

需要考虑以下几个问题：

1. 文件句柄个数：一般单个进程所管理的文件句柄是1024个，现在的linux系统可以修改这个上限
2. 系统内存问题：系统需要为每一个连接套接字开辟缓冲区（可能包括发送缓冲区和接收缓冲区），我们也可以通过配置每个缓冲区的大小还有缓冲区个数来管理缓冲区
3. 网络带宽：如果是1w个连接，每个连接每秒传输1kb的数据的话，那么带宽需要10000 * 1kb * 8 = 80Mbps

#### 解决思路：

解决的思路主要分为两个部分：

1. 第一个就是要解决操作系统用户态和内核态之间的交互问题，如何感知I/O事件的发生，如何在操作系统内核和应用程序之间交互数据
2. 第二个层面就是如何分配操作系统的进程和线程资源来服务大量的连接

##### 方式1：阻塞I/O + 进程

为每一个新连接fork出一个子进程，来管理新连接的服务。由于是一个独立的子进程负责处理一个连接中的所有I/O，所以即使是阻塞I/O，多个连接之间也不会互相影响。

但是这种方法的效率不高，扩展性差，每个子进程都占用一部分系统资源很难有大作为

##### 方式2：阻塞I/O + 线程

为每一个新连接通过pthread_create创建一个新的线程，来处理连接中的所有I/O请求。

因为连接不是随时都需要线程响应的，所以也可以通过线程池来管理线程，提高效率

##### 方式3：非阻塞I/O + readiness notification + 单线程

由于每一个连接有I/O的需求只是一小段的时间，并不需要时刻都有一个进程或者一个线程来照顾它，所以我们可以使用单线程来轮询所有接入的连接，判断每个连接是否有I/O需求，伪代码如下：

```c
for fd in fdset {
    if (is_readable(fd) == true) {
        handle_read(fd)
    } else if(is_writable(fd) == true) {
        handle_write(fd)
    }
}
```

如果是这种方式，如果接入的连接很多，每次轮询就会造成很多系统开销，造成效率很低，这是需要配合多路复用的技术，让操作系统告诉应用程序哪个套接字可以写，哪个套接字可以读，每次对注册了需要检测的fd进行检测，有之前学到的select和poll方法

伪代码如下：

```c
while(true) {
    poller.dispatch();
    for fd in registered_fdset {
        if (is_readable(fd) == true) {
            handle_read(fd);
        } else if (is_writable(fd) == true) {
            handle_write(fd);
        }
    }
}
```

还是慢，epoll可以让轮询检测连接的效率更快：在每次dispatch调用返回之后，只返回有I/O事件或者I/O事件发生变化的套接字：

```c
while(true) {
    poller.dispatch();
    for fd_event in active_event_set {
        if (is_readable_event(fd_event) == true) {
            handle_read(fd_event);
        } else if (is_writable_event(fd_event) == true) {
            handle_write(fd_event);
        }
    }
}
```

##### 方式4：非阻塞I/O + readiness notification + 多线程

基于之前的做法，将单线程改为多线程，使CPU的每一个核都作为一个I/O的分发器进行I/O事件分发

被称为主从reactor模式，这个深了

##### 方式5：异步I/O + 多线程

操作系统执行完成后回调







# Netty

在基于TCP的socket程序中，我们在网络编程协议中主要需要处理的是socket的**创建，listen，accept，connect，read，write**几种操作

netty使用协议和逻辑相分离，允许我们通过接口来实现，当协议事件发生时，我们该如何处理我们的逻辑，主要包含三个部分：

<img src="/Users/lizhigen/Desktop/截屏2021-07-07 下午2.24.16.png" alt="截屏2021-07-07 下午2.24.16" style="zoom:50%;" />

* Eventloop：主要负责任务执行与事件的检测
* channel：主要负责协议的建立与协议事件的处理
* Bootstrap：主要负责服务的建立与发布

### Netty.docs

#### 1. Before Getting Started

#### 2. Writing a Discard Server

**第一步：**首先需要实现一个handler，用于处理I/O事件

```java
package DiscardServer;

import io.netty.buffer.ByteBuf;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;
import io.netty.util.ReferenceCountUtil;

/*
handles a server-side channel
 */
public class DiscardServerHandler extends ChannelInboundHandlerAdapter {    // 通过继承ChannelInboundHandlerAdapter并重写其中的方法，我们可以实现处理不同事件的逻辑

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {    // 在这里重写关于接收数据的处理方法，每次read的时候会采用重写的方法中的逻辑
        // discard the receive data silently😂
        ((ByteBuf)msg).release();   // 这里实现discard接收到的消息
        /*
        ByteBuf是一个引用计数对象（reference-counted object），需要使用release方法进行显示地丢弃
        请记住，release所有传递给handler的引用计数对象是handler的责任。
         */
    }
}

```

引用计数对象顾名思义就是使用引用计数方法来管理垃圾回收的对象，我们需要负责引用计数对象的释放

一般对于传入数据的处理 + 对于引用计数对象的release handler基本形式会写成这样：

```java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    try {
        // Do something with msg
    } finally {
        ReferenceCountUtil.release(msg);
    }
}
```

exceptionCaught()方法，当Netty出现exception时被调用，exception可能包括I/O error或者我们实现的handler在处理事件时抛出异常。在大部分情况下，被捕获的异常需要被logged，与之相关的channel需要被关闭。也可以自己对抛出异常时的状况进行处理，比如，可能你想发送一个response message with an error code before closing he connection

```java
@Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        super.exceptionCaught(ctx, cause);
    }
```

**接下来：**编写启动一个Discard Server的方法

```java
package DiscardServer;

import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioServerSocketChannel;

/**
 * Created with IntelliJ IDEA.
 * User: lzg
 * Date: 2021/7/7
 * Time: 3:35 下午
 * messge:
 */

/*
Discard server: Discard any income data.
 */
public class DiscardServer {

    private int port;

    public DiscardServer(int port) {
        this.port = port;
    }

    public void run() throws Exception {
        /*
        NioEventLoopGroup是一种多线程的event loop，用于处理I/O操作
        Netty提供了多种多样的EventLoopGroup来应对不同的传输情况
        我们这里因为要实现一个server-side的应用，所以我们采用了两个EventLoopGroup
        一个是boss event group，接收传入的连接connection
        一个是worker event group，处理boss接受了并注册在连接中的connection
        这个多线程的event loop具体要有多少个线程，并且他们如何映射到创建的channels上都是基于我们如何实现这个eventloopGroup
        甚至可以在构造函数中直接配置
         */
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            /*
            serverBootstrap是一个helper class用来创建一个server
            不用helper可以使用Channel直接进行创建，但是这是一个乏味的过程(a tedious process)不建议这么干
             */
            ServerBootstrap serverBootstrap = new ServerBootstrap();
            /*
            1. 我们使用NioServerSocketChannel创建用于接收新传入连接的channel
            2. 每当有新的channel被接受，就会执行这个childHandler
               ChannelInitializer帮助用户配置新的Channel：你可能希望向新的channel的ChannelPipeline中添加例如之前创建的DiscardServerHandler
               当应用程序变得复杂之后，你可能会添加更多的handlers到这个pipeline中，并且最终将这个匿名类extract into a top-level class
            3. 4. option是parent group接受新的connection时的选项设置，childOption是child group对parent group已经接受了的connection的处理时的选项
            具体的ChannelOption需要看Netty的ChannelOption文档
             */
            serverBootstrap.group(bossGroup, workerGroup)
                    .channel(NioServerSocketChannel.class)  // 1.
                    .childHandler(new ChannelInitializer<SocketChannel>() { // 2.
                        @Override
                        protected void initChannel(SocketChannel socketChannel) throws Exception {
                            socketChannel.pipeline().addLast(new DiscardServerHandler());
                        }
                    })
                    .option(ChannelOption.SO_BACKLOG, 128)  // 3.
                    .childOption(ChannelOption.SO_KEEPALIVE, true); // 4.

            // bind and start to accept incoming connections
            /*
            我的注释：
            这个netty启动server好像不需要将socket转化为listen套接字，转化完了之后用这句就可以直接接收新的连接？不懂，先这么用，可能和Java的网络编程有关系
            */
            ChannelFuture channelFuture = serverBootstrap.bind(port).sync();
            
            // wait until the server socket is closed
            // in this example, this does not happen, but you can do that to gracefully shut down your server
            channelFuture.channel().closeFuture().sync();
        } finally {
            workerGroup.shutdownGracefully();
            bossGroup.shutdownGracefully();
        }
    }

    public static void main(String[] args) throws Exception {
        int port = 8080;
        if (args.length >0) {
            port = Integer.parseInt(args[0]);
        }
        new DiscardServer(port).run();
    }
}
```

可以将之前写的channelRead handler进行改造看看我们的server是否正确收到（丢弃）了信息

```JAVA
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    ByteBuf in = (ByteBuf) msg;
    try {
        while (in.isReadable()) { // 可以将这个低效的循环直接改为：System.out.println(in.toString(io.netty.util.CharsetUtil.US_ASCII))
            System.out.print((char) in.readByte());
            System.out.flush();
        }
    } finally {
        ReferenceCountUtil.release(msg); // 可以将这里改为in.release();
    }
}
```

#### 3. Write an Encho Server

echo server的搭建启动和之前discard server一样，不同点就是他们对于处理可读事件的handler不一样，只需要将handler进行修改：




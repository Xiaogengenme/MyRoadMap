* ORMs
* Transaction
* Data Replication
* Sharding Strategies
* CAP Theorem
* Database Normalization
* Indexes and how they work



# 《MySQL实战45讲》笔记

## 01 MySQL的基础架构：查询操作的执行流程是什么？

主要分为Server层与存储引擎层

Server层：包括连接器、查询缓存、分析器、优化器和执行器。所有跨存储引擎的功能都在这一层来实现，包括存储过程，触发器，视图等。

存储引擎层：默认为innoDB

#### 1、连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。

```shell
mysql -h$ip -P$port -u$user -p
```

建立连接后系统会自动读取并设置用户的权限，连接成功后就算管理员更改当前用户的权限也不会对本次连接产生影响，在下次该用户连接时才会发生权限变化。

![截屏2021-01-04 下午6.12.59](/Users/xiaogengen/Library/Application Support/typora-user-images/截屏2021-01-04 下午6.12.59.png)

使用show processlist；命令可以查看当前连接状态，Sleep表示空闲连接

空闲连接太长时间不操作就会自动断开，通过wait_timeout参数控制，默认为8小时

连接断开后客户端再次发送请求的话就会收到一个错误提醒：Lost connection to MySQL server during query.需要再次重连

数据库中分为长连接和短连接，尽量使用长连接，因为连接操作过程比较复杂。但是使用长连接可能会出现MySQL占用内存太大的情况，因为MySQL在执行过程中临时使用的内存是保存在连接对象里面的，这些资源只有在连接断开时才会释放。所以长期累积可能会造成OOM，现象就是MySQL异常重启了。

解决方法可以考虑：

1. 定期断开长连接
2. 如果使用MySQL5.7或更新版本，可以在执行完一次比较大的操作后使用mysql_reset_connection来重新初始化连接资源。这个过程可以将连接恢复到刚刚创建的状态而不需要重连和权限验证。

#### 2、查询缓存

MySQL8.0之后被删掉了

#### 3、分析器

词法分析与语法分析：词法分析分析出一条MySQL语句中哪个代表操作方法，哪个代表表名哪个代表列名；语法分析负责分析输入的MySQL语句是否有错误。

#### 4、优化器

优化器是在表里面有多个索引的时候选择使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

```mysql
select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;
```

可以先从表1选出c=10的记录的ID值，再根据ID值关联到表2，再从t2中选取d=20

也可以从表2开始关联到表1

优化器会选择这两种方法效率更高的一种来执行。

#### 5、执行器

```mysql
select * from T where ID=10;
```

执行器首先判断用户对表T是否有查询的权限

如果有权限就会调用innoDB引擎，一行一行地判断是否符合查询约束。

## 02 日志系统：更新操作的执行流程是什么？

创建一个表（主键ID和int字段c）

```mysql
create table t1(ID int primary key, c int);
```

将ID=2的那一行c增1

```mysql
update t1 set c=c+1 where ID=2;
```

#### redo log日志模块

WAL机制：Write Ahead Logging：**先写日志，再写磁盘**

redo日志模块大小固定，使用双指针进行管理：write pos是指要写入的位置，写到末尾后循环到头部，checkpoint是擦除的位置，之前的要保存到磁盘后擦除。

> redo log的存储位置：在内存中创建固定大小的赊账小黑板，写不下了就将check point前的内容擦除并存入磁盘

当write pos追上check pos时就要更新赊账白板更新checkpoint

<img src="https://static001.geekbang.org/resource/image/b0/9c/b075250cad8d9f6c791a52b6a600f69c.jpg" alt="img" style="zoom:60%;" />

#### binlog日志模块

redo log被innoDB设计在存储引擎层，binlog是MySQL自带的Server层中的方式。

redo log是物理日志，存储的是“ID=2位置的c需要更改成1”，而binlog是逻辑日志，会存储更新数据的逻辑“把ID=2位置的c加1”

redo log是固定大小循环写入的，binlog不是可以一直增加。

#### 两阶段提交

1. 执行器调用存储引擎的读取接口，获取ID=2这一行的数据。（存储引擎判断这个数据页是否在内存中，如果不在内存中需要取磁盘中获取）
2. 执行器对这行数据进行+1操作，更新完成后调用存储引擎的写接口写入数据。
3. 存储引擎将新数据写入内存，并将新更新写入redo log，此时redo log状态为prepare，存储引擎通知执行器更新已完成，随时可以commit
4. 执行器写binlog，写完后提交
5. 存储引擎提交事务，redo log状态更新为commit

> 存储引擎负责写redolog，redolog是存储由存储引擎管理存储在内存中的，相当于小赊账板
>
> 而binlog是由MySql Server层管理，由执行器读写的阶段性大日志文件

* 如果出现了误删库😭，需要做的是：

1. 首先找到最近一次的完整的数据库备份（叫**全量备份**），下载成本地临时库
2. 在这个本地临时库执行当时到误删库之前的binlog
3. 提交本地临时库

* **如果执行顺序是redo log prepare-redo log commit-binlog：**

如果在redolog写完之后发生了crash，redo log具有crash-safe可以成功提交更新的数据，但是binlog中没有写入更新数据的操作逻辑，所以在下载了本地临时库再执行binlog时，数据无法完成更新。

* **如果执行顺序是binlog-redo log prepare-redo log commit：**

如果redo log没执行成功就crash，数据不会更新仍是0，那么恢复数据时会执行binlog将数据+1，变成1与***原数据？？？（谁定义的原数据，直接更新不就行了？）***不符。

> 我觉得这里的原数据是指磁盘里本来应该有的数据，存储引擎没更新这条数据，这条数据不应该加一，在恢复时加一了与预期不符合。

* **参数：**

redo log：innodb_flush_log_at_trx_commit设成1的时候所有的数据更新都会直接持久化到磁盘中

binlog：sync_binlog设成1的时候也是一样

建议都设成1



## 03 事务隔离：修改的数据什么时候可以看见？

事务的特性：ACID：Atomically原子性 Consistency一致性  Isolation隔离性 Durability持久性

**事务的隔离性与数据库的效率**是对立权衡的

#### 四种事务隔离：

* 读未提交（Read Uncommitted）：事务A读取事务B未提交的数据。
* 读提交（Read Committed）：事务A读取事物B提交后的数据。***<u>读提交的视图是在SQL语句开始执行时创建的。？？？</u>***
* 可重复读（）：事务A在提交前所看到的数据不变，保持一致。需要在事务A启动时创建视图，之后一直查看这个视图。
* 串行化：设置独占式的读写锁，不允许并行读写

<img src="https://static001.geekbang.org/resource/image/7d/f8/7dea45932a6b722eb069d2264d0066f8.png" alt="img" style="zoom:50%;" />

以这张图为例：读未提交是在B未提交之前A就可以读到B更新的数据，所以v1v2v3都是2

读提交是在B提交之后A才可以读到B更新的数据，所以v1是1，v2v3是2

可重复读是指在A启动时，就创建一个查询数据的视图，前后数据保持一致，v1v2都是1，在A提交之后，读取v3为2

***串行化？？？***

#### 事务隔离的设置

transaction-isolation参数

#### 事务隔离的实现

#### 事务的启动方式

* 显式地启动事务。begin/start transaction/commit/rollback
* set autocommit=0，关闭掉自动提交，这样我们执行sql语句时，事务自动启动但是不会自动提交，需要手动执行commit或者关闭数据库才会提交

#### 长事务

set autocommit=0会造成select全部都存在事务中，造成长事务

***<u>建议set autocommit=1并且手动进行commit？？？</u>***



> 如何避免长事务？



### B+树

数据库索引

数据库两个最重要的任务：

```mysql
select * from user where id=2;							// 等值查找
select * from user where id>5 and id<=10;		// 区间查找
```

数据库索引主要解决以上两个问题，并且保证执行效率和存储空间。

#### 散列表：

散列表可以实现O(1)的查找速度和更新速度，但是不能实现区间查找

#### 平衡二叉搜索树：

二叉搜索树也可以实现快速的查找，前提是使其保持平衡，虽然中序遍历可以得到从小到大顺序的数据，但是也同样很难实现区间查找

#### 跳表：

跳表是一个很好的的实现方式，等值查找区间查找都可以实现，redis中有很好的跳表区间实现，之后再学习。

#### B+树——改造后的二叉查找树

B+树对二叉查找树进行了一下改造：

* 将二叉查找树中的节点中存储索引，而不是存储数据
* 将二叉查找树的叶子节点用链表进行连接，以满足区间查找

**问题：**将索引存储到二叉查找树节点中，二叉树节点较大（相较于一个索引字符），普通内存很难满足大型数据存储

很简单，将二叉查找树存在磁盘中。

**有问题：**内存的读写速度以纳秒计时，但是磁盘的读写速度只能以毫秒计时，每个节点的访问都对应着一次磁盘IO操作，那么查找一个数据需要的磁盘IO次数就等于树的高度，大大降低了效率，该如何解决？

将二叉查找树改为m叉查找树，比如一个包含16个索引的数据表，二叉查找树的高度为5层，需要4次磁盘IO操作（根节点存储在内存中），但是五叉查找树的高度为3层，只需两次磁盘IO操作。如果是100叉查找树，那么对于1亿条数据也只需要2次磁盘IO操作。

**还有问题：**如何确定m是多少呢？
$$
PAGE SIZE = (m-1)*4[keywords 大小]+m*8[children 大小]
$$
根据磁盘页的大小来确定m的大小，**尽量保证每个节点可以保存在一个页中**，读取一个节点正好需要一次磁盘操作。

```java
/**
 * 这是 B+ 树非叶子节点的定义。
 *
 * 假设 keywords=[3, 5, 8, 10]
 * 4 个键值将数据分为 5 个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF)
 * 5 个区间分别对应：children[0]...children[4]
 *
 * m 值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：
 * PAGE_SIZE = (m-1)*4[keywordss 大小]+m*8[children 大小]
 */
public class BPlusTreeNode {
  public static int m = 5; // 5 叉树
  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间
  public BPlusTreeNode[] children = new BPlusTreeNode[m];// 保存子节点指针
}
 
/**
 * 这是 B+ 树中叶子节点的定义。
 *
 * B+ 树中的叶子节点跟内部结点是不一样的,
 * 叶子节点存储的是值，而非区间。
 * 这个定义里，每个叶子节点存储 3 个数据行的键值及地址信息。
 *
 * k 值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：
 * PAGE_SIZE = k*4[keyw.. 大小]+k*8[dataAd.. 大小]+8[prev 大小]+8[next 大小]
 */
public class BPlusTreeLeafNode {
  public static int k = 3;
  public int[] keywords = new int[k]; // 数据的键值
  public long[] dataAddress = new long[k]; // 数据地址
 
  public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点
  public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点
}

```

<img src="/Users/xiaogengen/Library/Application Support/typora-user-images/image-20210219110137502.png" alt="image-20210219110137502"  />

 

**还有问题：**如果增加数据使每个节点块数量超过m怎么办？

超过m从下向上分裂，小于m/2就合并***（需要深入理解？？？）***

#### B树/B-树：

B树存储数据，B+树只存储索引

B树的叶子节点不需要用链表来串联

#### 思考

> B+树叶子节点使用的是单链表还是双向链表？



> 将散列表中的元素使用链表串联起来，是否能够实现区间查找呢？





### 04 数据库索引（上）

#### 索引模式

1. 散列表
2. 有序数组：插入数据很难，适用于静态数据集
3. B+树

#### 主键索引与非主键索引

* 主键索引的叶子节点存储的是整行数据，在innoDB中也被称为聚簇索引（cluster index）
* 非主键索引存储的是对应的主键索引，也叫二级索引（secondary index），通过二级索引查找数据时首先要定位到主键索引，再得到数据。

#### 索引的维护

设计之前提到的页分裂与页合并

#### 自增主键

NOT NULL PRIMARY KEY AUTO_INCREMENT

自增主键的好处是新增索引都是在末尾添加，并不涉及到向之前的页内插入，所以避免了很大一部分的索引维护的问题。

> 以身份证号为例，身份证可以唯一标识一条数据，那么是否还需要自增主键呢？

有了自增主键，将身份证号作为二级索引是不错的选择，因为二级索引内存储的是主键，自增主键的大小一般较小，存储在二级索引占用的空间较小，查询速度也较快。***（但是没说能不能就直接用身份证号做主键？为什么不呢？？？少了一次查询嘛不是？？）***

> 嘿嘿这个为请看第09节，唯一索引与普通索引的选择

非要用业务字段做主键也不是不行，有时要求只能有一个主键，而且某业务字段必须是主键（霸道）：叫做KV场景。

#### SQL学习：删除与重建索引

```mysql
alter table T drop index k;
alter table T add index(k);
```

```mysql
alter table T drop primary key;
alter table T add primary key(id);
```

> 这样重建索引的操作是合理的嘛？

这样重建索引目的是好的，但是执行语句不好。因为数据的增删改，索引表可能经历了很多次的分裂与合并，使得产生了许多碎片化的表空间，内存的利用率不高，重新建立索引表是为了让索引更加有序，查找更加快速，空间利用更加合理。

但是不需要这样的执行语句，因为执行下面一句其实已经是删除+重建，使得上面一句的执行是多余的，所以，可以用下面的sql代替：（以后会讲为什么）

```sql
alter table T engine=InnoDB;
```



### 05 索引（下）

![img](https://static001.geekbang.org/resource/image/dc/8d/dcda101051f28502bd5c4402b292e38d.png)

对于上面的表执行以下sql语句：

```sql
select * from t1 where k between 3 and 5;
```

数据库引擎会首先搜索到索引k为3的位置，然后通过k=3对应的主键在数据库中获取数据，然后向后遍历，得到5符合条件再通过主键获取数据，然后向后遍历6不符合结束select。

通过二级索引k定位到主键索引获取数据的方式叫做**回表**， 回表会影响效率，尽量避免。

> 如何减少回表次数提高效率呢？

#### 索引覆盖

比如上面的语句如果改成

```sql
select ID from t1 where k between 3 and 5;
```

这样使用索引k就可以解决。

> 执行这个操作MySQL在表上扫描了多少行？？？没搞懂是个问题

举一个覆盖索引很有用处的例子：在市民信息表中添加一个姓名与ID的联合索引

```sql
create table 'tuser' (
	'id' int(11) not null,
  'id_card' varchar(31) default null,
  'name' varchar(31) default null,
  'age' int(11) default null,
  'ismale' tinyint(1) default null,
  primary key ('id'),
  key 'id_card_name'('id_card','name')
  key 'name'('name')
)engine=innoDB;
```

注意以上是建立一个联合索引使用的语句，可以看到分别建立了关于姓名的索引，和身份证号-姓名的联合索引。

索引太多会影响数据库的性能效率，但是如果这个数据库频繁用到通过身份证号id_card查询name，id_card_name这个联合索引的价值就很大了。

**总结下来使用索引覆盖的含义就是使用联合索引的方式，用索引来代替从数据库中查询数据。（如果都不是主键就要用联合索引，如果通过二级索引查主键就不需要联合索引了）**

但建立索引的消耗与查询频率的权衡，需要DBA的聪明才智。

#### 最左前缀原则

![img](https://static001.geekbang.org/resource/image/89/70/89f74c631110cfbc83298ef27dcd6370.jpg)

联合索引可以通过开头一部分的内容进行左前缀的查询，这样可以提高查询的效率。

比如：查找所有姓张的人的ID：

```sql
select ID from t1 where name like "张%";
```

最左前缀可以是联合索引的最左m个字段，也可以是字符串索引最左m个字符。

可以想到的是这时联合索引的顺序就很重要了。

#### 索引下推

索引下推可以结合最左前缀原则来进行二次筛选：

比如查找所有姓张的，年龄为十岁的男孩的信息：

```sql
select * from tuser where name like "张%" and age=10 and ismale=1;
```

MySQL新加入索引下推的特性可以加快查询过程：

MySQL5.6之前并不会通过联合索引的值判断姓名和年龄，它只会通过姓名来关联主键，然后通过主键查询，需要很多次回表。

MySQL5.6之后加入了索引下推的特性，判断了姓张的人之后还会在联合索引中判断出年龄为10的人，再关联主键查询，减少了很多次回表！！









## 06 全局锁与表锁

#### 1. 全局锁

* 加全局锁的方法：

flush tables with read lock (FTWRL)

* 全局锁的使用场景：做全库逻辑备份

* 给全库加锁的问题：

1. 如果是主库加全局锁，那么整个主库都不能使用系统停摆
2. 如果是从库加全局锁，那么整个加锁期间就接受不到主库传来的binlog

* 那么如何做全局备份呢？

官方自带逻辑备份工具：mysqldump，当mysqldump使用参数--single-transaction的时候，引擎会使用一致性读事务隔离机制生成一致性视图进行备份工作。

但很多数据库表并不支持事务（MyISAM不支持事务，垃圾）

#### 2. 表级别锁

表级别锁和表锁不一样！！Mysql中的表级别锁有两种：表锁和元数据锁（MetaData Lock，MDL）

##### 表锁

```mysql
lock table ... read;
lock table ... write;
```

需要注意，表锁lock table会限制其他线程的读写之外，还会限制本线程接下来的操作。

```sql
lock table t1 read, t2 write;
```

例如这一句，在限制其他线程对t1的写，和对t2的读写之外，也限制了本线程对于表的操作，只能读t1，能读写t2。

##### MDL

MDL的作用是保证读写的正确性，不用自己主动添加，而是数据库自动添加

当一个操作对数据库进行增删改查操作的时候会给表加读锁，当修改数据库表的结构的时候，会加写锁

由于MDL是系统自动加的，所以在使用中有可能会造成许多问题：比如给一个表加一个字段会导致整个库崩溃。

## 07 行锁

行锁：事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成之后才能进行更新

> 有时候描述锁描述得一塌糊涂，简单说人话就是这样子

#### 两阶段锁协议

在innodb中，行锁不是事务一开始就加上的，而是在需要时加上，并且不会马上释放，而是在事务提交了之后才释放。这个就是两阶段锁协议。

所以：如果一个事物中需要锁多个行，要把最有可能造成锁冲突，最有可能影响并发度的锁尽量往后放。

#### 死锁与死锁检测

当并发系统中不同的线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

举个死锁的例子！！

![img](https://static001.geekbang.org/resource/image/4d/52/4d0eeec7b136371b79248a0aed005a52.jpg)

> 之前举不出死锁的例子是因为不了解两阶段提交的概念，事务A的第一个操作获取第一行的行锁，第二个操作等待第二行的行锁，事务不结束这个锁时不会释放的，而B获取了第二行的行锁等待第一行的行锁，事务不结束也是不会释放的，这样就形成了死锁，两阶段才会形成死锁。

结局死锁的策略有两种：

1. 设置等待锁超时时间：超时时间通过innodb_lock_wait_timeout设置
2. 发起死锁检测：发现死锁后主动回滚死锁链条中的某个事务，方法是将innodb_deadlock_detect设置为on

两种方法各有各的问题：

1. 设置死锁等待超时时间，这个时长不好控制，默认是50s，太长了失去了并发操作的意义，太短的话会把很多普通的锁等待误认为死锁，造成很多操作失效
2. 第二种方法，每新加入一个线程就需要判断新加入之后是否会造成死锁，这个时间复杂度较高On，这样如果并发量较大时会造成CPU占用很高但是执行的事务很少的情况，大多数的CPU资源都被用在了死锁检测上

如何解决热点行的更新的性能问题呢？

1. 关闭死锁检测：emmm

2. 控制并发度：

   1. 在客户端控制，每个客户端限制并发线程数
   2. 消息中间件
   3. 在mysql数据库层面控制并发：修改mysql代码，使对相同行的更新在进入引擎之前排队

3. 将热点行拆成多行

   例如影院余额这一行并发度很高，那就把影院余额这一行拆成十行，余额总额是十行之和。

   当然，不是简单的拆成十行就可以的，还需要很多具体的设计，比如退票功能如果某一行的余额为0就需要加入更多的逻辑。

#### 思考题

如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到：

- 第一种，直接执行delete from T limit 10000;
- 第二种，在一个连接中循环执行20次 delete from T limit 500;
- 第三种，在20个连接中同时执行delete from T limit 500。





## 09 普通索引和唯一索引该如何选择？

想象一个场景：如果要创建一个市民表，每个市民的身份证号都是不一样的，可以将身份证号设置为一个唯一索引，但是身份证号太长了，作为索引可能会影响索引查询的性能，所以可以选择再创建一个普通索引。

该如何选择呢？

![](/Users/xiaogengen/Library/Application Support/typora-user-images/image-20210729131721717.png)

### 查询操纵的差异

查询k=5位置的这条数据时，我们会在B+树索引结构中首先定位这条数据。在B+树中按层查找，而在一条B+树索引中可以采用二分查找

* 对于唯一索引，我们查找完ID=500这条数据之后，因为只有一条，所以直接结束查找
* 对于普通索引，查询完（5，500）这条数据之后还会向后看，看看还有没有等于500的，没有就结束了。由于对于索引的读取是每次读取磁盘中的一页放在内存里，所以往后找的速度是很快的。除非500是最后一项那很倒霉还要读取一下磁盘，概率很低

所以这两种操作相差无几。*（但是没说回表的问题？？？找到了之后普通索引还会回表呀！！！*

### 更新操作的差异

#### change buffer原理：

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，**在不影响数据一致性的前提下**，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

#### change buffer merge

将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。

#### 什么条件下可以使用change buffer？

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。

因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。

#### 如何使用change buffer向数据库中插入一条新数据？

第一种情况是，**这个记录要更新的目标页在内存中**。这时，InnoDB的处理流程如下：

- 对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。

第二种情况是，**这个记录要更新的目标页不在内存中**。这时，InnoDB的处理流程如下：

- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。

#### change buffer适合的应用场景

change buffer在每个数据页被读进内存的时候会执行merge操作。更新操作还是会稍微费时的，对buffer空间的利用率也不高，如果想发挥change buffer的效率，我们每次在执行merge之前要尽可能在buffer空间内攒足够多的更新操作。

所以change buffer适用于写多读少的场景。

### 所以要怎么选？

回到我们文章开头的问题，普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能。

### change buffer和redo log

执行以下操作：

```mysql
insert into t(id,k) values(id1,k1),(id2,k2);
```

向表中插入两行，假设k1在内存中，k2不在内存中

那么执行的操作是：

1. 直接向内存中的表中插入(id1, k1)
2. 在change buffer中记录要想k2所在表插入(id2, k2)
3. redo log记录上面两个操作

然后再把这两条数据读出来的时候，由于k1只是存储在内存中可以直接读出。

k2不在内存中，读操作会将k2对应的页从磁盘中读出，并根据change buffer来进行更新，并返回正确的数据。

**redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。**

> 所以change buffer和redo log的功能不是很一样，但是都是为了减少向磁盘读写时的开销。



## 12 为什么有的时候数据库会“抖”一下

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”**。

### 1. 什么情况会引发数据库的flush过程呢？

1. 第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。

   这个场景，对应的就是InnoDB的redo log写满了。这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。

2. 第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。

   这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。

   你一定会说，这时候难道不能**直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了？这里其实是从性能考虑的**。如果刷脏页一定会写盘（***以这个为前提***），就保证了每个数据页有两种状态：

   - 一种是内存里存在，内存里就肯定是正确的结果，直接返回；
   - 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。
     这样的效率最高。

3. 第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。

   这种场景，对应的就是MySQL认为系统“空闲”的时候。

4. 第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。

   这种场景，对应的就是MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

### 2. 以上的情况对数据库性能的影响

第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为0。

第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。**InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：**

- 第一种是，还没有使用的；
- 第二种是，使用了并且是干净页；
- 第三种是，使用了并且是脏页。

InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，第一种情况：未被使用的页面很少。

接下来就是选择要舍弃干净页还是脏页，干净页在内存中能直接使用很方便，所以尽量别舍弃。

把脏页写进磁盘然后刷掉就好了！

刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。***（？？？这个是什么？为什么要写日志？？）***

### 3. InnoDB的刷脏页控制策略

首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。

这个能力，就是磁盘性能的“全力”，但是我们不能把全力都用来刷脏页，接下来，就要考虑InnoDB怎么控制引擎按照“全力”的百分比来刷脏页。

**设计策略控制刷脏页的速度，会参考哪些因素呢？**

如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是redo log写满。

参数innodb_max_dirty_pages_pct是脏页比例上限，默认值是75%

***具体的计算逻辑以后再看吧，暂时理解不了？？？***

InnoDB会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，**无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源并可能影响到了你的更新语句**，都可能是造成你从业务端感知到MySQL“抖”了一下的原因。

#### 一个会影响刷脏页性能的MySQL机制：

一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了。而MySQL中的一个机制，可能让你的查询会更慢：**在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉**；还可以继续蔓延！

找“邻居”这个优化在机械硬盘时代是很有意义的，可以**减少很多随机IO**。机械硬盘的随机IOPS一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升。

而如果使用的是SSD这类**IOPS比较高的设备**的话，我就建议你把innodb_flush_neighbors的值设置成0。因为这时候IOPS往往不是瓶颈，而“**只刷自己**”，就能更快地执行完必要的刷脏页操作，减少SQL语句响应时间。

在MySQL 8.0中，innodb_flush_neighbors参数的**默认值已经是0**了。

#### 思考：

一个内存配置为128GB、innodb_io_capacity设置为20000的大规格实例，正常会建议你将redo log设置成4个1GB的文件。

但如果你在配置的时候不慎将**redo log**设置成了1个100M的文件，会发生什么情况呢？又为什么会出现这样的情况呢？


* 网络分层协议
* HTTP、HTTPS
* DNS协议
* 浏览器

# 网络

## 网络架构分层

### 网络的七层结构包括什么？

物理层

数据链路层：包括两个子层：逻辑链路控制子层和介质访问控制子层

网络层

传输层

会话层

表示层

应用层

#### 应用层协议有哪些？



#### 网络为什么要分层？

![截屏2021-04-15 上午9.57.49](/Users/xiaogengen/Library/Application Support/typora-user-images/截屏2021-04-15 上午9.57.49.png)



## TCP协议

### TCP与UDP的区别

1. TCP是面向连接的协议，UDP是面向非连接的协议
2. TCP传输的数据是有状态，可控制的，保证了传输的可靠性
3. UDP传输是基于数据报的，TCP为了传输的可靠性，将IP包变成了字节流

### TCP的三次握手和四次挥手过程？

#### 三次握手的过程

#### 两次握手为什么不行？

> 三次握手是为了保证两端的接收能力和发送能力

考虑情况一个发起方发送第一个SYN没有送达，第二个SYN成功通过两次握手建立了连接，这次连接没有问题

但是断开连接之后如果接收端接收到了第一个SYN，接收端默认发出了ACK就默认建立了链接，这时候发送方这边是不具有接收能力的

#### 🌟四次挥手的过程：

> 流程问题，注意四次挥手时所传递的信息，FIN-ACK-FIN+ACK-ACK

以客户端主动释放断开连接消息为例：

1. 客户端首先向服务器端发送一个断开连接的申请FIN
2. 服务器端向客户端传回确认ACK

3. 服务器端把要发的数据发完之后，向客户端也发送一个断开连接请求
4. 客户端再传一个确认信息给服务器端ACK，随后等待两个MSL（最大报文生成时间）之后断开连接

#### 🌟四次挥手为什么要等待2MSL？

##### 为了确认接收端收到了断开连接的确认消息

等待两个报文最大生成时间的原因是确认对方成功收到ACK消息，ACK传达到服务器的最长时间是1MSL，如果超过这个时间服务器端给主动断开端重传FIN，重传的FIN生存时间也是1MSL，等待2MSL也是为了如果ACK丢失确认服务器端可以收到重传的FIN

##### 为了清空这次连接存在在网络空间中的所有数据包

如果client直接closed，然后又向server发起了一个新连接，我们不能保证这个新连接和刚关闭的连接的端口号是不同的。假设新连接和已经关闭的老端口号是一样的，如果前一次滞留的某些数据仍然在网络中，这些延迟数据会在新连接建立后到达Server，所以socket就认为那个延迟的数据是属于新连接的，数据包就会发生混淆。所以client要在TIME_WAIT状态等待2倍的MSL，这样保证本次连接的所有数据都从网络中消失。

### TCP为什么是可靠的传输协议？

### TCP的流量控制/滑动窗口

TCP流量控制是为了避免发送端发送数据的速度超过接受端的能力从而造成数据丢失的情况。

由滑动窗口协议（连续ARQ协议）实现。滑动窗口协议既保证了数据包无差错、有序接收，也实现了流量控制。主要的方式就是接收方返回的 ACK 中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送。

#### 流量控制是否会引发死锁？如何解决？

如果接收方给发送方发送了一个窗口大小为0的应答，发送方将不会继续传数据。但之后接收方向发送方发送窗口不为0的应答如果丢失，那么接收方等待发送方数据，发送方以为接收方窗口为0，就造成了死锁。

解决：发送方维持计时器，每次计时器到时间就主动询问接收方的窗口大小。

### TCP的拥塞控制

#### 如何判断网络是否拥塞

拥塞控制主要来避免两种现象：***包丢失和超时重传***。所以可以通过观察这两个现象是否有增多来判断网络是否拥塞。

主要通过观察网络的吞吐量

流量控制是作用在发送端和接收端防止数据丢失，而拥塞控制是作用于整个网络环境的，防止因为网络环境的限制造成数据的丢失，出错。

拥塞控制主要包括以下几种算法：

* 慢启动
* 拥塞控制
* 快恢复与快重传

#### 慢启动

慢启动算法的思路就是不要一开始就大量传输数据，要探测网络可承受的流量大小，一点点增大发送方拥塞窗口的大小。

如何增大：每个传输轮次的时间是RTT，慢启动算法每经过一个传输轮次就将拥塞窗口的大小加倍。

轮次的概念：

![img](https://pic2.zhimg.com/80/v2-54715533f093170d50f1ff1be39006e9_1440w.jpg)

同时，慢启动算法中还需要有一个变量是ssthresh，是慢启动算法和拥塞避免算法的切换点

#### 拥塞避免算法

拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。

##### 如果增长太快出现拥塞

> 重新设置阈值（出现拥塞时拥塞窗口大小的一半）和拥塞窗口的大小（重置1），然后执行慢开始算法

无论是在慢开始阶段还是在拥塞避免阶段，**只要发送方判断网络出现拥塞**（其**根据就是没有按时收到确认**，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以**都当做拥塞来处理**），就把慢开始阈值ssthresh设置为出现拥塞时的**发送窗口大小的一半**（但不能小于2）。然后**把拥塞窗口cwnd重新设置为1**，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。

> 快速重传算法和快恢复算法是一起使用的

#### 快重传算法

> 分为发送方和接收方

快重传要求接收方接收到一个失序的报文段时立刻发送重复确认，而不是等到自己发送数据时捎带确认。

要求发送方只要收到三个重复的确认就应该立即重传对方尚未收到的报文段，而不是等到重传计时器到期。

#### 快恢复算法

当发送端收到接收端发送的三个数据包的确认帧就说明要重传这个确认帧下一个序号的数据包。但是这个和超时重传有一定区别，因为三个ack都可以送到发送端，所以发生快重传的时候发送端会认为当前的情况不是很严重，不会直接使用拥塞避免算法将拥塞窗口缩小太多。而是：

将拥塞窗口cwnd大小减半，将阈值ssthresh设置为cwnd（不是减半之后）

这样就没有像拥塞避免算法那样从1开始。







### ARQ协议

自动重传协议，用于数据的错误纠正。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。

#### 停等ARQ（stop-and-wait）

#### 连续ARQ——回退N帧ARQ（Go-Back-N）

1. 接收端丢弃从第一个没有收到的数据包开始的所有数据包
2. 发送端收到NACK后，从NACK中指明的数据包开始重新发送

#### 连续ARQ——选择性重传ARQ协议（Selective Repeat）

1. 发送端连续发送数据包但对每个数据包都设有个一个计时器
2. 当在一定时间内没有收到某个数据包的ACK时，发送端只重新发送那个没有ACK的数据包







## HTTP协议

### HTTP请求

1. 请求行：包括请求方法 + 访问的URL + 协议类型
2. 首部
3. 请求正文实体

#### 请求方法包括什么？

Http1.0定义了三种请求方法：get、post和head

Http1.1增加了六种请求方法：delete、put、patch、connect、trace、options

* GET：
* Post
* Head
* Delete
* Put
* Patch
* Connect
* Trace
* Options

##### 🌟Get方法和Post方法的区别有什么？（需要之后慢慢理解）

* 后退和刷新时
* 是否能被浏览器缓存（做书签）
* 对于数据的要求：类型
* 对于数据的要求：长度
* 数据的可见性
* 编码类型
* 历史记录

##### Post方法和Put方法的区别

在实际实用中，POST往往是用来创建一个资源的，而PUT方法往往是用来修改一个资源的



#### 有哪些重要的首部字段？

* Accept-Charset：客户端可以接受的字符集
* Content-Type：正文的格式（JSON）
* Cache-control：控制缓存，里面会存放一个max-age数据，超过这个max-age的资源就需要向服务器集群获取，而不是缓存层。

##### 浏览器缓存读取

高并发场景下一些静态资源需要存储在缓存服务器中，以免所有数据一同刷新造成访问效率很低，比如大图片刷新不出来的情况。

所以对于不是很经常需要刷新的资源我们将其存放在缓存层中。

其中nginx负责管理动态资源和静态资源的获取控制，对于不常刷新的静态资源就访问vanish缓存层，动态资源访问redis缓存层

缓存过期的数据才会访问Tomcat应用集群。

![截屏2021-04-15 下午5.32.40](/Users/xiaogengen/Library/Application Support/typora-user-images/截屏2021-04-15 下午5.32.40.png)

* If-Modified-Since：如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源。如果没有更新，就不需要下载，节省带宽访问更快。（返回304Not Modified：缓存重定向）



### 🌟HTTP的访问流程？

#### 1. 构建HTTP请求

#### 2. HTTP请求的发送

1. 建立连接

HTTP面向连接的方式发送请求，通过stream二进制流的方式传输

到了TCP层，将二进制流打包成报文段进行发送，这里通过TCP的特性进行数据的可靠传输，应用层不用知道传输层的细节

2. 加TCP头

3. 加IP头，网络层传输

   IP层需要查看目标地址和自己是否在同一个局域网之内。如果是，就发送ARP协议来请求这个目标地址对应的MAC地址，然后将源MAC地址和目标MAC地址放入MAC头，发送出去即可；

   如果不在同一个局域网，就需要发送到网关，还需要发送ARP协议

4. 接收端收到数据返回ack

#### 3. 构建HTTP返回





### 输入一个url后发生什么？

1. 域名解析：DNS
2. TCP连接（SSL连接）
3. 发送HTTP请求：请求行-请求头-请求体
4. 服务器返回HTTP响应：响应行-响应头-响应体
5. 浏览器收到响应渲染页面
6. 断开连接

### DNS的解析过程

本机DNS缓存 - 本机配置的DNS Server - 根DNS - XX DNS Server

### 常见的状态码

* 1XX：提示信息，协议处理的中间状态

* 2XX：服务器成功处理了客户端的请求

  * [200 OK]：最常见的成功状态码，如果是非HEAD请求服务器返回的响应头会有body数据
  * [204 No Content]：成功，响应头没有body数据
  * [206 Partial Content]：HTTP分块下载或或断电续传，body内容不是全部只是一部分

* 3XX：重定向

  * [301 Moved Permanently]：永久重定向

  * [302 Temporary Redirect]：临时重定向，301、302都会使用location字段指向重定向URL

    **301永久重定向和302临时重定向的区别：**

    永久重定向指当前访问URI的网址的内容永久搬到另一个URI，浏览器下次访问的时候也是默认直接访问迁移的URI。临时重定向顾名思义。

  * [304 Not Modified]：缓存重定向

* 4XX：错误

  * [400 Bad Request]：客户端向服务端发送的报文存在错误
  * [403 Forbidden]：服务器端不允许客户端的访问
  * [404 Not Found]：服务器资源找不到

* 5XX：服务器端错误码

  * 500：服务器错误（笼统）
  * 501：服务器作为网管访问后端服务器时故障
  * 502：当前功能还没支持，即将开业
  * 503：服务器正忙

### HTTP123代的迭代

#### HTTP1.0和HTTP1.1之间的区别

1. **缓存处理**

   相较于HTTP1.0请求头里面使用if-modified-since和expires来做缓存判断的标准，HTTP1.1中新增了更多的缓存控制策略

2. **带宽优化以及网络连接的使用**

   HTTP1.0中有一次浪费带宽的行为，比如可能只需要某个对象的一个部分，但是会把整个对象传回来

3. **新增错误状态码**

4. **Host头的处理**

   在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。

5. **长连接**

   HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。

#### HTTP1.X的优化

1. **降低延迟**，针对HTTP高延迟的问题，SPDY优雅的采取了多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了HOL blocking的问题，降低了延迟同时提高了带宽的利用率。
2. **请求优先级**（request prioritization）。多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。
3. **header压缩。**前面提到HTTP1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。
4. **基于HTTPS的加密协议传输**，大大提高了传输数据的可靠性。
5. **服务端推送**（server push），采用了SPDY的网页，例如我的网页有一个sytle.css的请求，在客户端收到sytle.css数据的同时，服务端会将sytle.js的文件推送给客户端，当客户端再次尝试获取sytle.js时就可以直接从缓存中获取到，不用再发请求了。

#### HTTP2.0对HTTP1.X的优化

- **新的二进制格式**（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。
- **多路复用**（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。
- **header压缩**，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。
- **服务端推送**（server push），同SPDY一样，HTTP2.0也具有server push功能



### HTTPS

#### 🌟HTTP和HTTPS区别

HTTP和HTTPS最大的区别就是安全性的区别，HTTP是使用明文传输，而HTTPS是使用密文传输的。

要生成密文就涉及到加密算法以及密钥的生成，所以HTTPS较HTTP的TCP协议基础上增加了SSL协议，所以建立连接的握手机制也比HTTP更复杂一些。

#### HTTPS目的是解决HTTP哪些问题？

1. 窃听风险
2. 篡改风险
3. 冒充风险（中间人攻击）

#### HTTPS如何解决上述问题

1. 采用混合加密的方式来实现信息的机密性，防止信息被窃听的风险
2. 摘要算法来实现校验数据的完整性，解决被篡改的风险
3. 身份验证机制防止冒充的风险

#### 混合加密

> 非对称加密：公钥加密私钥解密。客户端向服务器端申请服务器端的公钥，使用公钥对数据进行加密传输，服务器端收到密文后使用自己的私钥进行解密。

* 在通信建立之前采用非对称加密的方式
* 在通信建立之后采用对称加密来传输数据

#### 摘要算法

摘要算法通过给数据生成独一无二的指纹来校验数据的完整性，解决数据被篡改的风险。

发送方通过摘要算法算出要传输数据明文的指纹，并将指纹+明文一同加密成密文传输给接收方，接收方收到密文解密后通过相同的摘要算法计算明文的指纹，并与传输过来的指纹进行对比

#### 数字证书

**解决的问题**：非对称加密建立连接的过程中，客户端需要向服务器端申请它的公钥，用于后续加密消息传递密文，但这次申请没有安全保证，无法确认公钥的信任度。

借助第三方权威机构CA（数字证书认证机构），将服务器公钥放在数字证书中。

### 🌟HTTPS的建立连接过程

HTTPS的加密传输涉及到加密算法，加密算法有两种，对称加密和非对称加密。而HTTPS使用的是混合加密的方式，在简历连接时采用非对称加密，一旦连接建立，随后的通信使用对称加密进行传输。

HTTPS建立连接的过程主要有以下三个步骤：客户端验证服务器端的CA证书-双方协商生成对话密钥-双方使用密钥进行加密通信。

具体来说有四步：

1. clientHello，客户端向服务器端发送加密传输请求，传输的内容包括客户端的SSL协议版本，客户端支持的加密算法和一个随机数
2. serverHello：服务器端回应客户端，传输内容主要包括服务器端的CA证书，服务器端选择的加密算法，和第二个随机数
3. 客户端接下来通过操作系统或浏览器来验证服务器端的CA证书身份，如果验证成功则向服务器发送确认信息，包括第三个随机数，加密算法改变通知（要从非对称加密改为对称加密），和客户端握手结束通知（会将之前涉及的所有数据打包通过摘要算法生成一个最终结果）
4. 服务器端收到第三个随机数后通过三个随机数计算出本次通话的加密密钥，并向客户端传回：加密算法更改信息，和握手结束通知（数据摘要）



### cookie与session的区别

cookie主要用于存放用户的信息，以便用户下次登录的时候可以更快速地添加一下基本信息。

一般网页实现第二次直接登录的方法就是为用户信息生成一个token存放在用户的cookie中，下次就可以直接登录（为了安全每次重新登录都会更新token）

同时cookie还满足用户在访问一个网站的其他页面是也无需重新登录。

session主要用于保存用户状态，比如用户的购物车信息等。

session一般存放在服务器中，cookie一般存放在内存中（浏览器中）



### 网络层，源IP和目的IP在一个局域网内如何发送，不在的时候怎么发送



## RCP

## ![截屏2021-04-16 下午6.08.41](/Users/xiaogengen/Library/Application Support/typora-user-images/截屏2021-04-16 下午6.08.41.png)



#### 基本流程：

当客户端的应用想发起一个远程调用时，它实际是通过本地调用本地调用方的 **Stub。它负责将调用的接口、方法和参数，通过约定的协议规范进行编码**，并通过本地的 RPCRuntime 进行传输，将调用网络包发送到服务器。 

服务器端的 RPCRuntime 收到请求后，交给**提供方 Stub 进行解码**，然后调用服务端的方法， 服务端执行方法，返回结果，提供方 Stub 将返回结果编码后，发送给客户端，客户端的 RPCRuntime 收到结果，发给**调用方 Stub 解码得到结果，返回给客户端**。

#### 设计思想

这里面分了三个层次，对于用户层和服务端，都像是本地调用一样，专注于业务逻辑的处理就可以了。对于 Stub 层，处理双方约定好的语法、语义、封装、解封装。对于 RPCRuntime，主要 处理高性能的传输，以及网络的错误和异常。

